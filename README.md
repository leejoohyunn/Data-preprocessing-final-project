# Data-preprocessing-final-project
# 하이퍼파라미터 튜닝
## 1.1 하이퍼파라미터의 역할과 중요성
### 1.1.1 하이퍼파라미터와 모델 매개변수의 차이 (Hyperparameter vs. Parameter)
- 파라미터(매개변수): 모델이 데이터에서 규칙을 학습하는 데 사용되는 변수이며 훈련 과정에서 알고리즘에 의해 업데이트 됩니다. 파라미터에 대해 최적의 값을 설정하지 않지만, 데이터에서 자체 값을 학습합니다. 파라미터의 최적 값이 찾아지면 모델은 훈련을 마칩니다.
- 하이퍼파라미터(초매개변수): 모델 훈련을 제어하는 변수입니다. 따라서 하이퍼파라미터는 파라미터의 값을 제어할 수 있습니다. 즉, 파라미터의 최적 값은 사용하는 하이퍼파라미터의 값에 따라 달라집니다. 파라미터와는 달리, 하이퍼파라미터는 데이터에서 값을 학습하지 않고, 모델을 훈련하기 전에 수동으로 지정해야 합니다. 일단 지정되면 하이퍼파라미터 값은 모델 훈련 중에 고정됩니다. 
(https://medium.com/data-science-365/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82)
(https://velog.io/@emseoyk/%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D)

| 하이퍼파리미터                             | 파라미터                                          |
|------------------------------------------:|--------------------------------------------------:|
|학습 과정에 반영되는 값, 학습 시작 전 미리 조정| 데이터로부터 학습 또는 예측되는 값, 모델 내부에서 결정|
|ex) 학습률, 손실 함수, 배치 사이즈            |선형회귀 계수, 가중치, 편향, 평균, 표준편차           |
|직접 조정 가능                               |직접 조정 불가능                                    |

## 1.2 하이퍼파라미터의 종류
1. **학습률(Learning Rate)**: 모델이 한 번의 학습 단계에서 얼마나 많이 학습하는지를 조절합니다. 혹은, 손실함수(정답과 예측값 차이를 계산한 함수)를 최소화하는 파라미터를 찾는것이라고 할 수 있습니다. 아래 그림 같은 손실함수의 포물선에서 경사를 따라 이동하는 양입니다. 너무 작은 학습률은 수렴을 느리게 할 수 있고, 너무 큰 학습률은 발산의 위험을 가지고 있습니다.
![학습률 이미지](https://github.com/leejoohyunn/images/blob/main/image.png)


![학습률 이미지](https://github.com/leejoohyunn/images/blob/main/img.png)


3. **배치 크기(batch size)**: 모델이 각 학습 단계에서 처리하는 데이터 샘플의 개수를 나타냅니다. 적절한 배치 크기를 선택하는 것은 모델의 효율성에 영향을 미칩니다. 최적화를 진행하다가 극소값 혹은 안정점에 빠질 수 도 있어 크기가 크면 무조건 빠르고 효과적으로 최적화가 이뤄지는 것은 아닙니다. 배치 크기가 크면, 데이터의 평균적인 특성을 바탕으로 학습이 진행돼 gradient(기울기)를 크게 바꾸지 못합니다. 다시 말하면, 평균이 구해지면 특이값이 묻혀 영향력이 작아집니다. 반대로, 배치 사이즈가 작을 때는 이 구간을 빠져나오기 비교적 수월하다. 배치 사이즈는 학습 속도와 학습 성능에 모두 영향을 미치는 중요한 요소입니다. 이를 고려해 최적의 배치 사이즈를 부여해야합니다. 배치 크기는 보통 2의 제곱수를 사용합니다. CPU와 GPU의 메모리가 2의 배수여서 2의 제곱 수 일 경우에 데이터 송수신의 효율을 높일 수 있습니
   
![배치 사이즈 이미지](https://github.com/leejoohyunn/images/blob/main/%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C.png
)
![배치 사이즈 이미지](https://github.com/leejoohyunn/images/blob/main/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202023-12-17%20160749.png)
https://wikidocs.net/55580

> **미니 배치**
> 미니 배치란, 전체 데이터를 N등분해 각각의 학습 데이터를 배치 방식으로 학습시킨다. 즉, 전체 데이터 세을 몇 개의 데이터셋으로 나누었을 때, 그 작은 데이터 셋의 뭉치입니다. 미니 배치를 사용하는 이유는 데이터가 많을 때, 길어지는 시간이나 데이터의 손실을 줄이기 위해서 입니다.
https://welcome-to-dewy-world.tistory.com/86

5. **에포크 수(Number of Epochs)**: 전체 데이터셋을 한 번 훈련하는 것을 1 에포크라고 합니다. 에포크 수는 전체 데이터셋을 몇 번 반복해서 훈련할지를 결정합니다. 에포크 수를 높일수록, 다양한 무작위 가중치를 학습하는 것으로, 적합한 파라미터를 찾을 확률이 올라갑니다. 하지만, 에프크를 지나치게 높일 경우, 학습 데이터가 과적합되어 다른 데이터를 적용했을 때 제대로된 예측을 못합니다. 에포크는 학습 데이터셋 샘플의 수와 동일하게 하며 이는 배치수와 배치 사이즈를 곱한 값과 같습니다.

7. **가중치 감소(Weight Decay)**: 과적합을 방지하기 위해 가중치 감소를 사용합니다. 이는 가중치 값이 너무 크지 않도록 제한하는 역할을 합니다. 가중치 감소에는 규제(Regularization)이 이용된다. Regularization 이란 wieght의 절대값을 작게 만들며, weight의 모든 원소를 0에 가깝게해 특성이 출력에 주는 영향을 최소화로 만든다. 즉, 오버피팅되지 않도록 모델을 제한한다는 것이다. 대표적인 Regularization으로는 L1과 L2가 있다. 

>**L1 규제**: 각 weight의 제곱합에 규제 강도(Regularization) λ를 곱하고 그 값을 loss function(손실함수)에 더한다. λ를 크게 하면 가중치가 감소되고, λ를 작게하면 가중치가 증가한다. 일반적으로 L2 규제가 많이 쓰인다. 

 >**L2 규제**: weight의 제곱의 합이 아닌 가중치 합을 더해 regualrization strength λ를 곱해 오차에 더한다. L2 규제와 달리 L1 규제할 때는 일부 가중치 값이 0이된다. 이를 통해 모델에 대한 이해도가 높아지고, 모델에서 중요한 feature이 무엇인지 알 수 있다.
>https://goatlab.tistory.com/124

![배치 사이즈 이미지](https://github.com/leejoohyunn/images/blob/main/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202023-12-17%20170336.png)
https://sacko.tistory.com/45

9. **드롭아웃 비율(Dropout Rate)**:드롭아웃은 학습 중에 무작위로 일부 뉴런을 제외하여 모델의 일반화 성능을 향상시키는 데 사용됩니다. 드롭아웃 비율은 제외될 뉴런의 비율을 나타냅니다.일반적으로 학습할 때만 드롭아웃을 사용하고, 예측시에는 사용하지 않는다. 학습할 때 인공 신경망이 특정 뉴런 또는 특정 조합에 의존적이게 되는것을 방지해준다.

![배치 사이즈 이미지](https://github.com/leejoohyunn/images/blob/main/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202023-12-17%20170445.png)

>**앙상블 기법(Ensemble)**:
>여러 모델을 종합적으로 고려해 최적의 결과를 찾는것이다. 학습할 수 있는 장비가 많을 때 사용하는 방법으로, 다수의 돗립적인 학습 모델을 만들어 각자 학습한 뒤 모델들을 합쳐 한 번의 예측을 만드는것입니다. 이러한 점에서 앙상블은 드롭아웃과 유사하다는 것을 알 수 있습니다. 
https://childult-programmer.tistory.com/44
>

11. **활성화 함수(Activation Function)**:딥러닝 네트워크에서 노드에 입력된 값들을 비선형 함수에 통과시킨 후 다음 레이어로 전달하는데, 이때 활성화함수를 사용한다. 비선형 함수를 사용하는 이유는 딥러닝 모델의 레이어 층을 깊게 구성할 수 있기 때문이다. 활성화 함수의 종류로는 sigmoid 함수, Tanh 함수, ReLU 함수, Leaky ReLU, PReLU, ELU, Maxout 등이 있다.
>**시그모이드 함수**:
>
>특징:
   범위: (0, 1)
   출력이 0 또는 1에 가까워지면 그래디언트 소실 문제 발생 가능.
>
>사용:
   이진 분류 문제의 출력층에서 주로 사용.

>**Tanh 함수**:
>
>특징:
   범위: (-1, 1)
   시그모이드와 유사하지만 출력 범위가 더 넓어 그래디언트 소실 문제가 상대적으로 줄어듦.
>
>사용:
   이진 분류 문제나 RNN(순환 신경망)에서 활성화 함수로 사용.

>**ReLU**:
>
>특징:
   양수 입력에 대해 선형, 음수 입력에 대해 0을 출력.
   학습이 빠르고 계산 효율성이 뛰어남.
>
>사용:
   컨볼루션 신경망 (CNN) 및 이미지 인식과 같은 분야에서 주로 사용.
   주의: 입력이 음수인 경우, 그래디언트 소실 문제가 발생할 수 있음.

>**Leaky ReLU**:
>
>특징:
   음수 입력에 대해 작은 기울기를 가진 선형 함수.
   ReLU의 문제를 해결하기 위해 도입.
>
>사용:
   일반적으로 ReLU의 대안으로 사용.

>**PReLU**:
>
>특징:
   Leaky ReLU의 확장으로 음수 입력에 대해 학습 가능한 기울기를 가짐.

>사용:
   데이터셋에 따라 Leaky ReLU보다 더 나은 성능을 보일 수 있음.

>**ELU**:
>
>특징:
   음수 입력에 대해 작은 기울기를 가진 지수 함수.
   ReLU의 문제를 완화하면서 그래디언트 소실 문제를 줄임.
>
>사용:
   신경망의 히든 레이어에서 성능이 좋을 수 있음.

>**Maxout**:
>
>특징:
   두 개의 입력 중 더 큰 값을 선택.
   매개변수가 더 많아 계산 비용이 높을 수 있음.
>
>사용:
   매우 깊은 네트워크에서 특히 성능 향상을 위해 사용.
https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=handuelly&logNo=221824080339
![활성화함수 이미지](https://github.com/leejoohyunn/images/blob/main/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202023-12-17%20181409.png)

13. **최적화 알고리즘(Optimizer)**: 딥러닝 학습시 손실함수의 최솟값을 찾아가는 것을 최적화(Optimization)이라고 하며, 이를 수행하는 알고리즘이 최적화 알고리즘(Optimizer)이다. 모델의 가중치를 업데이트하는 데 사용되는 최적화 알고리즘을 선택합니다. 대표적으로는 SGD, Adam, RMSprop 등이 있습니다.
https://velog.io/@freesky/Optimizer
15. **은닉층 수와 뉴런 수**:신경망의 구조를 결정하는 하이퍼파라미터로, 은닉층의 수와 각 은닉층의 뉴런 수를 조절합니다.
16. **합성곱 신경망(CNN)에서의 커널 크기와 스트라이드**:이미지 분류와 같은 작업에서 사용되는 CNN에서는 커널 크기와 스트라이드를 조절하여 특징을 추출하는 방식을 결정합니다.
17. **랜덤 시드(Random Seed)**:랜덤 초기화를 사용하는 경우, 랜덤 시드는 동일한 조건에서 실험을 재현하기 위해 사용됩니다.

(https://doug.tistory.com/44)

## 1.3 튜닝 방법 소개
## 1.4 실제 적용 사례

## 1.5
